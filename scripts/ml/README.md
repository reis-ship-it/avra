# Machine Learning Training Scripts

This directory contains scripts for training the calling score neural network model (Phase 12 Section 2).

## Files

### Core Scripts
- **`train_calling_score_model.py`**: Main training script that trains the neural network model and exports it to ONNX format
- **`train_outcome_prediction_model.py`**: Trains outcome prediction binary classifier model
- **`optimize_calling_score_model.py`**: Hyperparameter optimization script for finding best model configurations
- **`export_training_data.dart`**: Exports real training data from Supabase to JSON format

### Data Generation
- **`generate_synthetic_training_data.py`**: Generates 100% synthetic training data for initial model development and testing
- **`generate_hybrid_training_data.py`**: Generates hybrid training data using Big Five converted profiles (real personality data + synthetic spots/context/timing)

### Architecture
- **`dataset_base.py`**: Base dataset architecture with `TrainingDataset`, `TrainingRecord`, and `DatasetMetadata` classes for consistent data structures across all generators
- **`model_manager.py`**: Manages model downloading, verification, and registration

## Quick Start

### Option 1: Generate 100% Synthetic Data (for testing)

```bash
# Generate 10,000 synthetic samples
python scripts/ml/generate_synthetic_training_data.py \
  --num-samples 10000 \
  --output-path data/calling_score_training_data.json
```

### Option 2: Generate Hybrid Data (Recommended - uses Big Five personality data)

```bash
# Step 1: Download Big Five dataset from Kaggle
python scripts/knot_validation/download_sample_dataset.py \
  --kaggle tunguz/big-five-personality-test \
  --output data/raw/big_five.csv

# Step 2: Convert Big Five to SPOTS format
python scripts/knot_validation/data_converter.py \
  data/raw/big_five.csv \
  --output data/raw/big_five_spots.json

# Step 3: Generate hybrid training data (real personality + synthetic spots/context/timing)
python scripts/ml/generate_hybrid_training_data.py \
  data/raw/big_five_spots.json \
  --output data/calling_score_training_data_hybrid.json \
  --num-samples 10000
```

### 2. Train Model

```bash
# Train model with default settings
python scripts/ml/train_calling_score_model.py \
  --data-path data/calling_score_training_data.json \
  --output-path assets/models/calling_score_model.onnx

# Train with custom settings
python scripts/ml/train_calling_score_model.py \
  --data-path data/calling_score_training_data.json \
  --output-path assets/models/calling_score_model.onnx \
  --epochs 200 \
  --batch-size 64 \
  --learning-rate 0.0001
```

### 3. Export Real Data (optional)

```bash
# Set environment variables
export SUPABASE_URL="your_supabase_url"
export SUPABASE_ANON_KEY="your_supabase_anon_key"

# Export data
dart scripts/ml/export_training_data.dart \
  --output-path data/calling_score_training_data.json
```

## Requirements

### Python Dependencies

```bash
pip install torch numpy scikit-learn
```

### Dataset Architecture

All training data generators use the shared dataset architecture from `dataset_base.py`:

- **`TrainingRecord`**: Single training record with user/spot vibes, context, timing, and outcomes
- **`DatasetMetadata`**: Dataset provenance, statistics, and generation parameters
- **`TrainingDataset`**: Complete dataset with metadata and records, includes validation and statistics calculation

This ensures consistency across synthetic, hybrid, and real data generators.

### Data Format

The training data JSON file follows this structure (generated by `TrainingDataset`):

```json
{
  "metadata": {
    "num_samples": 10000,
    "description": "..."
  },
  "training_data": [
    {
      "user_vibe_dimensions": {
        "exploration_eagerness": 0.75,
        "community_orientation": 0.60,
        ...
      },
      "spot_vibe_dimensions": {
        "exploration_eagerness": 0.80,
        ...
      },
      "context_features": {
        "location_proximity": 0.70,
        ...
      },
      "timing_features": {
        "optimal_time_of_day": 0.65,
        ...
      },
      "formula_calling_score": 0.75,
      "is_called": true,
      "outcome_type": "positive",
      "outcome_score": 0.80
    },
    ...
  ]
}
```

## Model Architecture

- **Input**: 39 features
  - User vibe dimensions (12D)
  - Spot vibe dimensions (12D)
  - Context features (10 features)
  - Timing features (5 features)
- **Architecture**: MLP (39 → 128 → 64 → 1)
- **Output**: Calling score (0.0-1.0)

## Training Process

1. **Data Loading**: Load training data from JSON file
2. **Preprocessing**: Normalize features using StandardScaler
3. **Data Splitting**: Split into train (70%), validation (15%), test (15%)
4. **Training**: Train model with early stopping
5. **Evaluation**: Evaluate on test set
6. **Export**: Export trained model to ONNX format

## Output

The training script outputs:
- **ONNX Model**: `assets/models/calling_score_model.onnx` (or custom path)
- **Training Metrics**: Printed to console (train loss, validation loss, test loss)

## Next Steps

After training:
1. Place the ONNX model in `assets/models/calling_score_model.onnx`
2. The Flutter app will automatically load and use the model
3. Monitor model performance through A/B testing framework

## Notes

- **Synthetic Data**: Use 100% synthetic data for initial development and testing
- **Hybrid Data**: Use hybrid data (Big Five personality + synthetic spots/context/timing) for better personality distributions
- **Real Data**: Use real data from Supabase for production model training
- **Minimum Data**: At least 1,000 samples recommended for initial testing, 10,000+ for production
- **Model Updates**: Retrain model periodically as more real data becomes available

## Data Generation Comparison

| Approach | User Vibes | Spot Vibes | Context/Timing | Outcomes | Quality |
|----------|-----------|------------|----------------|----------|---------|
| **100% Synthetic** | Synthetic | Synthetic | Synthetic | Synthetic | ⭐⭐ |
| **Hybrid (Big Five)** | Real (Big Five) | Synthetic | Synthetic | Synthetic | ⭐⭐⭐ |
| **Real SPOTS Data** | Real | Real | Real | Real | ⭐⭐⭐⭐⭐ |

**Recommendation**: Use hybrid data for v1.0 model (better than 100% synthetic), then retrain on real SPOTS data for v2.0+ (best quality).
