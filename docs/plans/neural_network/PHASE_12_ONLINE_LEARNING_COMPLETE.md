# Phase 12: Online Learning Implementation - Complete

**Date:** December 28, 2025  
**Status:** âœ… Complete  
**Section:** 3.2.1: Continuous Learning

---

## âœ… **Implementation Complete**

### **Core Component Created**

**`OnlineLearningService`** (`lib/core/services/online_learning_service.dart`)
- Manages periodic retraining of neural network models
- Collects new training data from outcomes
- Triggers retraining based on time or data thresholds
- Auto-registers new model versions after retraining
- Tracks model performance metrics over time

---

## ğŸ“‹ **Features Implemented**

### **1. Scheduled Retraining**
- âœ… Weekly retraining interval (configurable)
- âœ… Daily checks for retraining triggers
- âœ… Automatic retraining when enough new data available
- âœ… Manual retraining trigger support

### **2. Data Collection**
- âœ… Counts new training data since last retraining
- âœ… Exports training data for Python retraining scripts
- âœ… Filters for records with outcomes (required for training)
- âœ… Minimum threshold: 1,000 new samples

### **3. Version Management Integration**
- âœ… Auto-registers new model versions after retraining
- âœ… Integrates with `ModelVersionManager`
- âœ… Sets new versions to `staging` status
- âœ… Starts with low weight (0.1) for safety

### **4. Performance Tracking**
- âœ… Daily performance tracking for all active versions
- âœ… Calculates metrics from A/B test outcomes:
  - Positive outcome rate
  - Average calling score
  - Average outcome score
  - Sample count
- âœ… Caches performance metrics per version

### **5. State Management**
- âœ… Tracks last retraining date
- âœ… Tracks last performance tracking date
- âœ… Prevents concurrent retraining
- âœ… Graceful error handling

---

## ğŸ”§ **Configuration**

### **Retraining Settings**
```dart
static const int minSamplesForRetraining = 1000; // Minimum new samples
static const Duration retrainingInterval = Duration(days: 7); // Weekly
static const Duration performanceTrackingInterval = Duration(hours: 24); // Daily
```

### **Retraining Triggers**
1. **Time-based**: Retrain every 7 days (weekly)
2. **Data-based**: Retrain when 1,000+ new samples available
3. **Manual**: Trigger retraining programmatically

---

## ğŸ”— **Integration**

### **Dependency Injection**
- âœ… Registered in `injection_container.dart`
- âœ… Dependencies: `SupabaseClient`, `CallingScoreDataCollector`, `ModelVersionManager`, `AgentIdService`

### **Data Sources**
- âœ… `calling_score_training_data` table (training data with outcomes)
- âœ… `calling_score_ab_test_outcomes` table (A/B test results for performance tracking)

### **Model Versioning**
- âœ… Integrates with `ModelVersionManager` for version registration
- âœ… Uses `ModelVersionRegistry` for version lookup
- âœ… Auto-creates new versions after retraining

---

## ğŸš€ **Usage Examples**

### **Initialize Service**
```dart
final onlineLearning = sl<OnlineLearningService>();
await onlineLearning.initialize();
// Service automatically starts scheduled retraining and performance tracking
```

### **Manual Retraining Trigger**
```dart
final success = await onlineLearning.triggerRetraining(
  modelType: 'calling_score',
  reason: 'manual',
);
```

### **Check Retraining Status**
```dart
if (onlineLearning.isRetraining) {
  print('Retraining in progress...');
}

final lastRetrain = onlineLearning.lastRetrainingDate;
print('Last retraining: ${lastRetrain ?? "never"}');
```

### **Get Performance Metrics**
```dart
final metrics = onlineLearning.getPerformanceMetrics('v1.1-hybrid');
if (metrics != null) {
  print('Positive outcome rate: ${(metrics.positiveOutcomeRate * 100).toStringAsFixed(1)}%');
  print('Sample count: ${metrics.sampleCount}');
}
```

### **Register New Model After Training**
```dart
// After Python training script completes and generates new ONNX model
await onlineLearning.registerNewModelVersion(
  version: 'v1.2-hybrid',
  modelPath: 'assets/models/calling_score_model_v1_2_hybrid.onnx',
  modelType: 'calling_score',
  trainingMetrics: {
    'test_loss': 0.0234,
    'val_loss': 0.0256,
    'training_samples': 15000,
  },
);
```

---

## ğŸ“Š **Workflow**

### **Automatic Retraining Flow**
```
1. Service checks daily for retraining triggers
   â†“
2. If 7 days passed OR 1000+ new samples:
   â†“
3. Export new training data
   â†“
4. Trigger Python retraining script (TODO: backend integration)
   â†“
5. After training completes:
   â†“
6. Register new model version (v1.2, v1.3, etc.)
   â†“
7. Update last retraining date
   â†“
8. New version available for A/B testing
```

### **Performance Tracking Flow**
```
1. Service tracks performance daily
   â†“
2. Query A/B test outcomes for last 7 days
   â†“
3. Calculate metrics:
   - Positive outcome rate
   - Average calling score
   - Average outcome score
   â†“
4. Cache metrics per version
   â†“
5. Update version performance metrics
```

---

## ğŸ”„ **Retraining Process**

### **Step 1: Data Collection**
- Service queries `calling_score_training_data` table
- Filters for records with outcomes since last retraining
- Exports to JSON format for Python scripts

### **Step 2: Training Trigger**
- Currently logs the trigger (TODO: backend integration)
- In production, would call:
  - Backend API endpoint
  - Cloud function (Firebase, AWS Lambda)
  - Local script execution (development)

### **Step 3: Model Registration**
- After training completes, call `registerNewModelVersion()`
- New version automatically registered in `ModelVersionRegistry`
- Status set to `staging` for A/B testing

### **Step 4: A/B Testing**
- Use `ModelVersionManager` to start A/B test
- Gradually increase traffic (10% â†’ 50% â†’ 100%)
- Monitor performance metrics

---

## ğŸ“ˆ **Performance Metrics Tracked**

### **Per Version Metrics**
- **Sample Count**: Number of outcomes tracked
- **Positive Outcome Rate**: % of positive outcomes
- **Average Calling Score**: Mean calling score
- **Average Outcome Score**: Mean outcome score
- **Tracking Period**: Days of data (default: 7)

### **Usage**
```dart
final metrics = onlineLearning.getPerformanceMetrics('v1.1-hybrid');
// Use metrics to:
// - Compare versions
// - Make deployment decisions
// - Adjust model weights
```

---

## ğŸš¨ **TODO: Backend Integration**

The service currently logs retraining triggers. For production, integrate with:

### **Option 1: Backend API**
```dart
// Call backend API to trigger retraining
await http.post(
  Uri.parse('https://api.spots.com/ml/retrain'),
  body: jsonEncode({
    'model_type': 'calling_score',
    'data_export_path': 'path/to/exported/data.json',
  }),
);
```

### **Option 2: Cloud Function**
```dart
// Trigger cloud function (Firebase, AWS Lambda, etc.)
await cloudFunctions.httpsCallable('triggerRetraining').call({
  'modelType': 'calling_score',
  'dataPath': 'path/to/data.json',
});
```

### **Option 3: Local Script (Development)**
```dart
// Execute Python script locally
final process = await Process.start(
  'python3',
  ['scripts/ml/train_calling_score_model.py', '--data-path', dataPath],
);
```

---

## âœ… **Success Criteria**

- [x] Online learning service created
- [x] Scheduled retraining implemented
- [x] Data collection and export implemented
- [x] Performance tracking implemented
- [x] Version management integration complete
- [x] Dependency injection configured
- [ ] Backend integration for actual retraining (TODO)
- [ ] Automated model deployment after retraining (TODO)

---

## ğŸ¯ **Next Steps**

### **Immediate**
1. **Backend Integration**: Connect retraining trigger to actual Python script execution
2. **Automated Deployment**: Auto-deploy new models after successful retraining
3. **Testing**: Test retraining workflow end-to-end

### **Future Enhancements**
1. **Incremental Learning**: Update model weights without full retraining
2. **Adaptive Thresholds**: Adjust retraining thresholds based on data quality
3. **Multi-Model Support**: Retrain both calling score and outcome models
4. **Performance-Based Retraining**: Retrain when performance degrades
5. **Rollback Automation**: Auto-rollback if new version performs worse

---

## ğŸ“ **Notes**

- **Retraining Frequency**: Weekly by default, adjustable via configuration
- **Data Threshold**: 1,000 new samples minimum (prevents overfitting on small datasets)
- **Version Naming**: Auto-increments (v1.1 â†’ v1.2 â†’ v1.3, etc.)
- **Safety**: New versions start with low weight (0.1) and `staging` status
- **Performance Tracking**: Daily tracking provides continuous monitoring

---

**Last Updated:** December 28, 2025  
**Status:** âœ… Complete - Ready for Backend Integration
